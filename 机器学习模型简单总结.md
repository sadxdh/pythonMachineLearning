# 机器学习模型简单总结

## 回归模型

### 线性回归模型

倘若一个模型的评估只与一个变量成相关性，那么我们就可以使用线性回归模型根据该变量模拟生成预测变量

类似一元变量方程，线性回归就相当于拟合出该映射关系的一般函数，在输入自变量时，近似预测出可能的应变量

#### 损失函数

- 损失函数（loss function）就是用来度量模型的预测值f(x)与真实值Y的差异程度的运算函数，**值越小通常说明模型对训练数据的拟合度越高**
- 相关链接：[损失函数（Loss Function）详解](https://zhuanlan.zhihu.com/p/261059231)

#### 过拟合

- [过适 - 维基百科，自由的百科全书 (wikipedia.org)](https://zh.wikipedia.org/zh-hans/過適)

#### 两种解决过拟合的方法

1、减少特征变量

2、正则化处理

##### 两种通过正则化防止过拟合的模型

##### 岭回归模型

- 岭回归模型：岭回归模型是一种改良的**最小二乘法**，是一种能够**避免过拟合**的线性模型

- 相关链接：[最小二乘法详解](https://zhuanlan.zhihu.com/p/38128785)

- 相关链接：[sklearn官网-岭回归](https://scikit-learn.org/stable/auto_examples/linear_model/plot_ridge_path.html)

  实验参考：[基于线性回归算法的预测模型](./4.基于线性回归算法的预测模型.jupyter)

- 相对于**线性回归模型**，岭回归模型**alpha=1.0**的均方误差并**没有减小**，但权重系数值有所**减小**

- 相对于**岭回归模型alpha=1.0**，岭回归模型**alpha=10**的均方误差**没有减小**，但权重系数值**减小**

- 相对于**岭回归模型alpha=1.0**，岭回归模型**alpha=0.1**的均方误差**有所减小**，但权重系数值**更大**

  

- 当alpha=1.0时，特征变量系数**普遍偏大**

- **当alpha=10时，特征变量系数大多在0附近，也就是说模型的复杂度大大降低，模型更不容易出现过拟合**

- 当alpha=0.1时，特征变量系数与**线性回归模型**重合，而线性回归模型并没有进行正则化处理，对应的特征变量系数值会**非常大**

- ![img](pics/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E7%AE%80%E5%8D%95%E6%80%BB%E7%BB%93/sphx_glr_plot_ridge_path_001.png)

###### 总结

alpha值越大，模型的复杂度就越低，模型越不容易出现过拟合

alpha值越小，模型输出的不确定性就越大

- 相关链接：[正则化为什么能防止过拟合（重点地方标红了）](https://www.cnblogs.com/alexanderkun/p/6922428.html)

##### 套索回归模型

- 套索回归模型：和岭回归类似，不同的是套索模型使用**L1正则化**。**L1正则化会导致模型有一部分特征变量的系数正好等于0**

### 伯努利朴素贝叶斯模型

二项分布

离散数据

### 高斯朴素贝叶斯模型

数据集满足高斯分布

### 多项式朴素贝叶斯模型

### 决策树模型



### 随机森林模型

### 支持向量机

### 神经网络

感知机

简单双层神经网络：（随机梯度下降法）

基本步骤：设置参数、数据生成、权重初始化、<前向传播、计算损失、反向传播、权重更新>

```python
import numpy as np
from numpy. random import randn
N, D_in, H, D_out = 64, 1000, 100, 10  # 参数设置
x, y = randn(N, D_in), randn(N, D_out)  # 数据生成
w1, w2 = randn(D_in, H), randn(H, D_out)  # 权重初始化
for t in range(2000):
    # 前向传播
    h =1/(1+ np. exp(-x.dot(w1)))
    y_pred = h.dot(w2)
    # 计算损失
    loss = np.square(y_pred- y).sum()
    print(t, loss)
    # 反向传播
    grad_y_pred = 2.0 * (y_pred - y)
    grad_w2 = h.T.dot(grad_y_pred)
    grad_h =grad_y_pred.dot(w2.T)
    grad_w1 = x.T.dot(grad_h * h *(1-h))
    # 权重更新
    w1-= 1e-4 * grad_w1
    w2-= 1e-4 * grad_w2
```



## 分类模型

### KNN模型

#### 原理

找到距离目标点最近的k（k最好取奇数）个点，根据k个点不同分类出现的频率，确定目标点的类别

**k值选取**：k过小容易导致过拟合，k过大导致预测错误率过高

**距离度量**：欧氏距离（直线距离）

**具体步骤**：计算距离，排序，选取前k个点，计算类别频率，找出频率最高的类别

#### 案例

假设我们现在红、绿、蓝三种颜色的点，分布在二维空间中，这就对应了分类任务中的训练样点包含了三个类别，且特征数量为2。

**一种情况**：如果现在我们希望**推测图中空心圆的那个点是属于那个类别**，那么knn算法将会计算**该待推测点**与**所有训练样点**之间的**距离**，并且**挑选出距离最小的k个样点**（此处设定k=4），则图中与连接的4个点将被视为推测空心点（待推测点）类别的参考依据。显然，由于**这4个点均为红色类别**，则该待推测点即**被推测为红色**类别。

![v2-3951bd4de76f33926df7c25e6f4cf2b6_720w](pics/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E7%AE%80%E5%8D%95%E6%80%BB%E7%BB%93/v2-3951bd4de76f33926df7c25e6f4cf2b6_720w.webp)

**第二种情况**：如果待推测点在中间的某个位置（如下图所示），则同样也计算出与其最邻近的4个样本点，而**此时这4个样本点包含了3个类别（1红、1蓝、2绿）**，针对这样的情况，knn算法通常采用**投票法**来进行类别推测，即找出k个样本点中类别出现次数最多的那个类别，因此该待推测点的类型值即被推测为绿色类别。

![v2-36efd019ba97c56c0c02520a728801cb_720w](pics/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E7%AE%80%E5%8D%95%E6%80%BB%E7%BB%93/v2-36efd019ba97c56c0c02520a728801cb_720w.webp)

机器学习中著名的没有免费午餐定理(no free lunch theorem),**对于所有的任意一个问题（学习任务）来说，并不存在最好的模型，**反之恰恰提醒了我们，**对于特定的学习任务，我们需要去考虑最适合该问题学习器**，也就是**具体问题具体分析**的哲学道理。

那么，knn也必然有其存在的价值，比如说，我们现在拿到一个学习任务，需要去选择一个学习器去解决该问题，而且也没有任何对该问题研究的前车之鉴，那么，从何下手？通常，我们不需要上来就用一个神经网络模型或者强大的集成学习模型去做，而是可以**先用一用简单模型做一下“试探”**，比如knn就是一个很好的选择，这样的“试探”的好处在哪里呢？我们知道，**knn本质上属于懒惰学习的代表**，也就是说**它根本就没有用训练数据去拟合一个什么模型**，而是**直接用top-k个近邻的样本点做了个投票就完成了分类任务**，那么：

- 如果这样一个懒惰模型在当前的问题上就已经能够得到一个**较高的精度**，则我们可以认为当前的学习任务是比较简单的，**不同类别的样本点在特征空间中的分布较为清晰**，无需采用复杂模型
- 反之，若knn得到的精度很低，则传达给我们的信息是：该学习任务有点复杂，往往伴随着的消息就是，当前问题中不同类别样本点在特征空间中分布不是清晰，通常是**非线性可分**的，需要我们去调用更强大的学习器。

从而，一个简单的knn机器学习算法恰恰可以**帮助建模者对问题的复杂度有一个大概的判断**，协助我们接下来如何展开进一步的工作：继续挖掘特征工程、或者是更换复杂模型等。

```python
# -*- coding: utf-8 -*-

import numpy as np
import operator

class KNN(object):

    def __init__(self, k=3):
        self.k = k

    def fit(self, x, y):
        self.x = x
        self.y = y

    def _square_distance(self, v1, v2):
        return np.sum(np.square(v1-v2))

    def _vote(self, ys):
        ys_unique = np.unique(ys)
        vote_dict = {}
        for y in ys:
            if y not in vote_dict.keys():
                vote_dict[y] = 1
            else:
                vote_dict[y] += 1
        sorted_vote_dict = sorted(vote_dict.items(), key=operator.itemgetter(1), reverse=True)
        return sorted_vote_dict[0][0]

    def predict(self, x):
        y_pred = []
        for i in range(len(x)):
            dist_arr = [self._square_distance(x[i], self.x[j]) for j in range(len(self.x))]
            sorted_index = np.argsort(dist_arr)
            top_k_index = sorted_index[:self.k]
            y_pred.append(self._vote(ys=self.y[top_k_index]))
        return np.array(y_pred)

    def score(self, y_true=None, y_pred=None):
        if y_true is None and y_pred is None:
            y_pred = self.predict(self.x)
            y_true = self.y
        score = 0.0
        for i in range(len(y_true)):
            if y_true[i] == y_pred[i]:
                score += 1
        score /= len(y_true)
        return score
```

```python
# -*- coding: utf-8 -*-

import numpy as np
import matplotlib.pyplot as plt
from knn import *


# data generation
np.random.seed(314)
data_size_1 = 300
x1_1 = np.random.normal(loc=5.0, scale=1.0, size=data_size_1)
x2_1 = np.random.normal(loc=4.0, scale=1.0, size=data_size_1)
y_1 = [0 for _ in range(data_size_1)]

data_size_2 = 400
x1_2 = np.random.normal(loc=10.0, scale=2.0, size=data_size_2)
x2_2 = np.random.normal(loc=8.0, scale=2.0, size=data_size_2)
y_2 = [1 for _ in range(data_size_2)]

x1 = np.concatenate((x1_1, x1_2), axis=0)
x2 = np.concatenate((x2_1, x2_2), axis=0)
x = np.hstack((x1.reshape(-1,1), x2.reshape(-1,1)))
y = np.concatenate((y_1, y_2), axis=0)

data_size_all = data_size_1+data_size_2
shuffled_index = np.random.permutation(data_size_all)
x = x[shuffled_index]
y = y[shuffled_index]

split_index = int(data_size_all*0.7)
x_train = x[:split_index]
y_train = y[:split_index]
x_test = x[split_index:]
y_test = y[split_index:]

# visualize data
plt.scatter(x_train[:,0], x_train[:,1], c=y_train, marker='.')
plt.show()
plt.scatter(x_test[:,0], x_test[:,1], c=y_test, marker='.')
plt.show()

# data preprocessing
x_train = (x_train - np.min(x_train, axis=0)) / (np.max(x_train, axis=0) - np.min(x_train, axis=0))
x_test = (x_test - np.min(x_test, axis=0)) / (np.max(x_test, axis=0) - np.min(x_test, axis=0))

# knn classifier
clf = KNN(k=3)
clf.fit(x_train, y_train)

print('train accuracy: {:.3}'.format(clf.score()))

y_test_pred = clf.predict(x_test)
print('test accuracy: {:.3}'.format(clf.score(y_test, y_test_pred)))

'''
train accuracy: 0.986
test accuracy: 0.957
'''
```

![image-20231010145951805](pics/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E7%AE%80%E5%8D%95%E6%80%BB%E7%BB%93/image-20231010145951805.png)

参考：[knn算法的原理与实现 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/36549000)

代码：https://github.com/leizhang-geo/machine_learning_algorithms/tree/master/basic_algorithm/knn

### 逻辑回归模型

#### 原理

**用一个映射函数Sigmoid将一个线性回归模型得到的连续结果映射到离散模型上**

#### 逻辑函数

逻辑函数也称**sigmoid函数**将数据分为0和1

- x趋向正无穷时，函数值趋于1
- x趋向负无穷时，函数值趋向0
- x趋向0时，函数值趋向1/2

```python
import numpy as np
import matplotlib.pyplot as plt
def sigmoid(x):
    return 1.0/(1+np.exp(-x))
 
sigmoid_inputs = np.arange(-10,10,0.1)
sigmoid_outputs = sigmoid(sigmoid_inputs)
# print("Sigmoid Function Input :: {}".format(sigmoid_inputs))
# print("Sigmoid Function Output :: {}".format(sigmoid_outputs))
 
plt.plot(sigmoid_inputs,sigmoid_outputs)
plt.xlabel("Sigmoid Inputs")
plt.ylabel("Sigmoid Outputs")
plt.show()
```

![image-20230922164532554](pics/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E7%AE%80%E5%8D%95%E6%80%BB%E7%BB%93/image-20230922164532554.png)

#### 优点

计算代价不高，易于理解和实现，速度快，存储资源占用少

#### 缺点

容易欠拟合，分类精度可能不高，在面对多元或者非线性决策边界时性能较差

### 伯努利朴素贝叶斯模型

### 多项式朴素贝叶斯模型

### 决策树模型

### 随机森林模型

### 支持向量机

### 神经网络



## 聚类模型

### 完美聚类和不可能三角

Kleinberg 2002年提出完美聚类算法需要满足以下三点：

- 同比例缩放数据，聚类结果不变
- 同组数据距离缩小，非同组数据距离放大
- 聚类算法需要能够灵活包含所有聚类的可能性

以及"不可能三角"

<img src="pics/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E7%AE%80%E5%8D%95%E6%80%BB%E7%BB%93/image-20231009174556476.png" alt="image-20231009174556476" style="zoom: 50%;" />

参考：Kleinberg J. An impossibility theorem for clustering[J]. Advances in neural information processing systems, 2002, 15.

### K-means

#### 1.伪代码

```伪代码
获取数据 n 个 m 维的数据
随机生成 K 个 m 维的点
while(t)
    for(int i=0;i < n;i++)
        for(int j=0;j < k;j++)
            计算点 i 到类 j 的距离
    for(int i=0;i < k;i++)
        1. 找出所有属于自己这一类的所有数据点
        2. 把自己的坐标修改为这些数据点的中心点坐标
end
```

时间复杂度：![image-20231009192756904](pics/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E7%AE%80%E5%8D%95%E6%80%BB%E7%BB%93/image-20231009192756904.png)，其中，t 为迭代次数，k 为簇的数目，n 为样本点数，m 为样本点维度。

空间复杂度： ![image-20231009192809176](pics/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E7%AE%80%E5%8D%95%E6%80%BB%E7%BB%93/image-20231009192809176.png)，其中，k 为簇的数目，m 为样本点维度，n 为样本点数。

#### 2. 优缺点

##### 2.1 优点

- 容易理解，聚类效果不错，虽然是**局部最优**， 但往往局部最优就够了；
- 处理大数据集的时候，该算法可以保证较好的**伸缩性**；
- 当簇近似**高斯分布**的时候，**效果非常不错**；
- 算法复杂度低。

##### 2.2 缺点

- **K 值需要人为设定**，不同 K 值得到的结果不一样；
- **对初始的簇中心敏感，不同选取方式会得到不同结果**；
- **对异常值敏感**；
- 样本只能归为一类，**不适合多分类任务**；
- **不适合太离散的分类、样本类别不平衡的分类、非凸形状的分类**。

#### 3. 算法调优与改进

针对 K-means 算法的缺点，我们可以有很多种调优方式：如数据预处理（去除异常点），合理选择 K 值，高维映射等。以下将简单介绍：

##### 3.1 数据预处理

K-means 的本质是基于欧式距离的数据划分算法，均值和方差大的维度将对数据的聚类产生决定性影响。所以未做归一化处理和统一单位的数据是无法直接参与运算和比较的。常见的数据预处理方式有：数据归一化，数据标准化。

此外，离群点或者噪声数据会对均值产生较大的影响，导致中心偏移，因此我们还需要对数据进行异常点检测。

##### 3.2 合理选择 K 值

K 值的选取对 K-means 影响很大，这也是 K-means 最大的缺点，常见的选取 K 值的方法有：手肘法、Gap statistic 方法。

手肘法：



![img](pics/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E7%AE%80%E5%8D%95%E6%80%BB%E7%BB%93/v2-5ca4a5fe0b06b25a2b97262abb401a16_720w.webp)



当 K < 3 时，曲线急速下降；**当 K > 3 时，曲线趋于平稳**，通过手肘法我们认为拐点 **3 为 K 的最佳值**。

详细参考：[【机器学习】K-means（非常详细） - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/78798251)

### DBSCAN

DBSCAN，一种密度聚类算法，常用于非线性或非球面数据集。**epsilon**和**minPts**是两个必需的参数。epsilon是附近数据点的半径，这些数据点需要被认为是足够“相似”才能开始聚类。最后，minPts是需要在半径内的数据点的最小数目。

无法聚类则变为噪声

![img](pics/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E7%AE%80%E5%8D%95%E6%80%BB%E7%BB%93/20180725173312977.jpg)

### Mean Shift

Mean Shift算法，又称为**均值漂移算法**，Mean Shift的概念最早是由Fukunage在1975年提出的，在后来由Yizong Cheng对其进行扩充，主要提出了两点的改进：

定义了核函数；
增加了权重系数。

**不断在感兴趣区域寻找质心。**核函数的定义使得偏移值对偏移向量的贡献随之样本与被偏移点的距离的不同而不同。权重系数使得不同样本的权重不同。Mean Shift算法在聚类，图像平滑、分割以及视频跟踪等方面有广泛的应用。

本质是KDE算法，可以用等高线看出密度

![这里写图片描述](pics/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E7%AE%80%E5%8D%95%E6%80%BB%E7%BB%93/20160511165109659.jpg)

### AGNES

**凝聚的层次聚类：**一种自底向上的策略，首先将每个对象作为一个簇，然后合并这些原子簇为越来越大的簇，直到某个终结条件被满足。（小类->大类）
**分裂的层次聚类：**采用自顶向下的策略，它首先将所有对象置于一个簇中，然后逐渐细分为越来越小的簇，直到达到了某个终结条件。（大类->小类）

![这里写图片描述](pics/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E7%AE%80%E5%8D%95%E6%80%BB%E7%BB%93/20170923211237983.jpg)