

# 数据预处理



![image-20231007191839427](pics/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/image-20231007191839427.png)

## 均值方差标准化(正太分布)

均值方差标准化是一种将数据转化为**标准正态分布**的标准化方法。

在**回归模型**中，服从正态分布的自变量和因变量往往对应着较好的回归预测效果。

均值方差标准化的计算公式为：![image-20231007160406175](pics/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/image-20231007160406175.png)

公式中，x表示单个数据的取值，μ \muμ表示对应列的均值，σ \sigmaσ表示对应列的标准差

```python
from sklearn.preprocessing import StandardScaler

# 使用StandardScaler进行数据处理
scaler = StandardScaler().fit(X)
X_1 = scaler.transform(X)
# 也可以使用StandardScaler.fit_transfrom(X)一步到位得到X_1

plt.scatter(X_1[:,0],X_1[:,1],c='blue')
plt.title('StandardScaler预处理后数据')
```

![image-20231007191912398](pics/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/image-20231007191912398.png)

数值全部调整到0的左右

**优点**

1) 计算相对简单，在计算机编程软件中操作方便；

2) 能够消除量级为数据分析带来的不便，不受数据量级的影响，**保证了数据间的可比性**.

**缺点**

1) 计算时需要得到总体的均值及标准差，**在数据较多时难以实现**，大多数情况下用样本均值及标准差代替，此举会导致分析结果与真实结果之间会存在差异；

2) 极大程度上改变了数据的原始意义，使得**只能比较数据之间的关系**，导致这种标准化方法的现实意义需要在**比较**中实现；

3) 对数据的相关性有要求，只有在数据大致符合正态分布时才能得到最佳结果.

## 离差标准化（0~1）

**数据集的数值范围变化非常大，需要将数值范围缩放到合理大小**

**不涉及距离度量、协方差计算、数据不符合正态分布**

离差标准化是对原始数据所做的一种线性变换，**将原始数据的数值映射到[0,1]区间**。



![image-20231007190226217](pics/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/image-20231007190226217.png)

```python
from sklearn.preprocessing import MinMaxScaler

# 使用StandardScaler进行数据处理
X_2 = MinMaxScaler().fit_transform(X)

plt.scatter(X_2[:,0],X_2[:,1],c='blue')
plt.title('MinMaxScaler预处理后数据')
```

![image-20231007192019084](pics/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/image-20231007192019084.png)

数值全部调整到0-1范围内

**优点**

方便数据的处理。消除单位影响及变异大小因素影响。

**缺点**

当有新数据加入时，可能会导致最大值最小值发生变化，需要重新计算

## 归一化处理（保留特征）

**常用**：分类、聚类

**优点**：防止过拟合

需要对特征向量的值进行调整时，**确保每个特征向量都缩放到相同的数值范围内**

**将样本在向量空间模型上进行转换**

Normalizer方法将所有样本的特征向量转化为欧几里得距离为1.

也就是说，它把数据的分布变成一个半径为1的圆，或者是一个球。（默认的L2参数）

Normalizer通常是在我们**只想保留数据特征向量的方向**，而**忽略其数值**的时候使用。

```python
from sklearn.preprocessing import Normalizer
# 使用Normalizer进行数据处理，默认范数为‘L2’
# 将所有样本的特征向量转化为欧几里得距离为1，通常只想保留数据特征向量的方向，而忽略其数值的时候使用
X_3 = Normalizer().fit_transform(X)

plt.scatter(X_3[:,0],X_3[:,1],c='blue')
plt.title('Normalizer预处理后数据，默认L2范数')
```



![image-20231007191556812](pics/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/image-20231007191556812.png)

```python
# 使用Normalizer进行数据处理，使用范数为‘L1’
X_4 = Normalizer(norm='l1').fit_transform(X)

plt.scatter(X_4[:,0],X_4[:,1],c='blue')
plt.title('Normalizer预处理后数据，L1范数')
```

![image-20231007191635837](pics/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/image-20231007191635837.png)

L1范数结果：数据点大致分布在(0,1)(1,0)之间。原理：将样本各特征值除以各个特征值的绝对值之和

## 二值化处理(0 and 1)

通过设置阈值，将数值转换为0和1

```python
import numpy as np
from sklearn.preprocessing import Binarizer

data = np.array([
    [3,-1.5,   2,-5.4],
    [0,   4,-0.3, 2.1],
    [1, 3.3,-1.9,-4.3]
])

data_binarized = Binarizer(threshold=1.4).transform(data)
data_binarized
'''
array([[1., 0., 1., 0.],
       [0., 1., 0., 1.],
       [0., 1., 0., 0.]])
'''
```

## PCA降维

将高维数据转换为低维数据，并保证数据点与原高维空间的关系保持不变或近似

去除噪声和不重要的特征，在一定的信息损失范围内，大大缩减模型拟合成本

**原理**：线性投影、线性降维

将高维的数据映射到低维的空间中表示，并期望在所投影的维度上数据的方差最大，以此使用较少的数据维度，同时保留较多的元数据点的特性。

**主要思想**：将n维特征映射到k维上（k<n），**在映射过程中要求每个维度的样本方差最大**，达到尽量使新的k维特征向量之间互不相关的目的。

这些数据中拥有方差最大的k各维度被成为**主成分**，是在原有n维特征的基础上重新构造出来的k维特征。

**PCA主要工作**：主要工作就是从原始的空间中有序地找一组相互正交的坐标轴，新的坐标轴的选择与数据本身是密切相关的。

**优点**：

- 大大降低数据复杂性，节约时间成本

- 防止高维数据导致的过拟合

**应用范围**：数据压缩、数据可视化、提升机器学习速度等

### 指定特征数的降维

将**对主元向量**的**重要性进行排序**，根据需要的**取前面最重要的部分**，将后面的维数省去，同时**最大程度地保留了原有数据地信息**

```python
# 指定特征数的降维
from sklearn.decomposition import PCA
# 指定保留的特征数为3
pca_num = PCA(n_components=3)
# 训练PCA模型
pca_num.fit(X)
# 对样本数据进行降维
X_pca1 = pca_num.transform(X)
# 查看降维结果
print('对iris数据集进行指定特征数的降维后的维度为：',X_pca1.shape)
print("对iris数据集进行指定特征数的降维后的前五行数据：\n",X_pca1[:5])
'''
对iris数据集进行指定特征数的降维后的维度为： (150, 3)
对iris数据集进行指定特征数的降维后的前五行数据：
 [[-2.68412563  0.31939725 -0.02791483]
 [-2.71414169 -0.17700123 -0.21046427]
 [-2.88899057 -0.14494943  0.01790026]
 [-2.74534286 -0.31829898  0.03155937]
 [-2.72871654  0.32675451  0.09007924]]
 '''
```

如果某个原始特征对应的数字是**正数**，说明它和该主成分是**正相关**，若是**负数**，则为**负相关**。
```python
import numpy as np
print("进行指定特征数的降维后的最大方差成分：")
for i in range(pca_num.components_.shape[0]):
    arr = np.around(pca_num.components_[i],2)
    print("components{0}:{1}".format((i+1),[x for x in arr]))
'''
进行指定特征数的降维后的最大方差成分：
components1:[0.36, -0.08, 0.86, 0.36]
components2:[0.66, 0.73, -0.17, -0.08]
components3:[-0.58, 0.6, 0.08, 0.55]
'''
```

```python
var = np.around(pca_num.explained_variance_,2)
print("进行指定特征数的降维后的各主成分的方差为：",var)
var_ratio = np.around(pca_num.explained_variance_ratio_,2)
print("进行指定特征数的降维后的各主成分的方差为：",var_ratio)
'''
进行指定特征数的降维后的各主成分的方差为： [4.23 0.24 0.08]
进行指定特征数的降维后的各主成分的方差为： [0.92 0.05 0.02]
'''
```

### 指定方差百分比降维

```python
# 指定保留的方差百分比为0.95
pca_per = PCA(n_components=0.95)
# 训练PCA模型
pca_per.fit(X)
# 对样本数据进行降维
X_pca2 = pca_per.transform(X)
# 查看降维结果
print("对iris数据集进行方差百分比的降维后的维度为：",X_pca2.shape)
print("对iris数据集进行方差百分比的降维后的前五行数据：\n",X_pca2[:5])
'''
对iris数据集进行方差百分比的降维后的维度为： (150, 2)
对iris数据集进行方差百分比的降维后的前五行数据：
 [[-2.68412563  0.31939725]
 [-2.71414169 -0.17700123]
 [-2.88899057 -0.14494943]
 [-2.74534286 -0.31829898]
 [-2.72871654  0.32675451]]
'''
```

数据集从四维降到了两维

```python
print("进行方差百分比降维后的最大方差成分：")
for i in range(pca_per.components_.shape[0]):
    arr = np.around(pca_per.components_[i],2)
    print("components{0}:{1}".format((i+1),[x for x in arr]))
'''
进行方差百分比降维后的最大方差成分：
components1:[0.36, -0.08, 0.86, 0.36]
components2:[0.66, 0.73, -0.17, -0.08]
'''
var = np.around(pca_num.explained_variance_,2)
print("进行指定特征数的降维后的各主成分的方差为：",var)
var_ratio = np.around(pca_num.explained_variance_ratio_,2)
print("进行指定特征数的降维后的各主成分的方差为：",var_ratio)
'''
进行指定特征数的降维后的各主成分的方差为： [4.23 0.24 0.08]
进行指定特征数的降维后的各主成分的方差为： [0.92 0.05 0.02]
'''
```

<2.红酒数据集可视化-数据预处理.ipynb>

## 独热编码（One-Hot Encoding）

在很多机器学习任务中，特征并不总是连续值，而有可能是**分类值**。

**离散特征**的编码分为两种情况：

1、离散特征的取值之间没有大小的意义，比如color：[red,blue],那么就使用one-hot编码

2、离散特征的取值有大小的意义，比如size:[X,XL,XXL],那么就使用数值的映射{X:1,XL:2,XXL:3}

### 值无大小区别

```python
import numpy as np
from sklearn.preprocessing import OneHotEncoder

data_type = np.array([
    [0,1],
    [1,3],
    [2,0],
    [1,2]
])
# 进行独热编码
encoder = OneHotEncoder(categories='auto').fit(data_type)
data_encoded = encoder.transform(data_type).toarray()
print("编码后的数据：\n",data_encoded)
'''
编码后的数据：
 [[1. 0. 0. 0. 1. 0. 0.]
 [0. 1. 0. 0. 0. 0. 1.]
 [0. 0. 1. 1. 0. 0. 0.]
 [0. 1. 0. 0. 0. 1. 0.]]
'''
# 第一列 有0，1，2三种，对应有三位的编码
# 第二列 有0，1，2，3四种，对应有四位编码
```

### 值有大小区别

分两步：先进行装箱操作，再进行独热编码

```python
# OneHotEncoder只能用于整数数据，如果是非整型数据，就需要先进行数据转换，然后进行独热编码
import numpy as np
# 定义一个随机数的数组
np.random.seed(38)
arr = np.random.uniform(-5,5,size=20)
# 设置箱体数为5
bins = np.linspace(-5,5,6) # 生成六个元素的等差数列，中间就有5个箱子，所有数据放在不同的区间（箱子）里面，用箱子号表示数据点所在组
# 将数据进行装箱操作，np.digitize函数将数值转换为分类型数组，也就是对数据进行离散化处理，或者装箱处理
target_bin = np.digitize(arr,bins=bins)
# 输出装箱数据范围
print('装箱数据范围：\n{}'.format(bins))
print('\n数据点的特征值：\n{}'.format(arr))
print('\n数据点所在组：\n{}'.format(target_bin))
# 将所有数放在不同的箱子里，至此生成了一组离散的值

# 接下来进行独热编码处理
from sklearn.preprocessing import OneHotEncoder
target_bin = target_bin.reshape(-1,1)
onehot = OneHotEncoder(sparse=False,categories='auto')
onehot.fit(target_bin)
# 使用独热编码转化数据
arr_in_bin = onehot.transform(target_bin)
# 输出结果
print('\n装箱编码后的数据维度：{}'.format(arr_in_bin.shape))
print('\n装箱编码后的数据值：{}'.format(arr_in_bin))
'''
装箱数据范围：
[-5. -3. -1.  1.  3.  5.]

数据点的特征值：
[-1.1522688   3.59707847  4.44199636  2.02824894  1.33634097  1.05961282
 -2.99873157 -1.12612112 -2.41016836 -4.25392719 -2.19043025 -0.61565849
 -0.16750956  3.68489486  0.29629384  0.62263144 -0.28944656  1.88842007
  0.04828605  3.23175755]

数据点所在组：
[2 5 5 4 4 4 2 2 2 1 2 3 3 5 3 3 3 4 3 5]

装箱编码后的数据维度：(20, 5)

装箱编码后的数据值：[[0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 1.]
 [0. 0. 0. 1. 0.]
 [0. 0. 0. 1. 0.]
 [0. 0. 0. 1. 0.]
 [0. 1. 0. 0. 0.]
 [0. 1. 0. 0. 0.]
 [0. 1. 0. 0. 0.]
 [1. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0.]
 [0. 0. 1. 0. 0.]
 [0. 0. 1. 0. 0.]
 [0. 0. 0. 0. 1.]
 [0. 0. 1. 0. 0.]
 [0. 0. 1. 0. 0.]
 [0. 0. 1. 0. 0.]
 [0. 0. 0. 1. 0.]
 [0. 0. 1. 0. 0.]
 [0. 0. 0. 0. 1.]]
'''
```




## 标记编码器（字符串数值化）

整理好数据集后发现数据集中即包含**字符串字段**也包含**数值字段**

目标：将字符串字段数据转换为数值形式

```python
# 导入LabelEncoder类
from sklearn.preprocessing import LabelEncoder
# 定义一个标记编码器
label_encoder = LabelEncoder()
# 创建一些标记
input_classes=['audi','ford','audi','toyota','ford','bmw'] # 有重复值
# 为标记编码
label_encoder.fit(input_classes)
# 输出编码结果
print('Class mapping:')
for i,item in enumerate(label_encoder.classes_):
    print(item,'-->',i)
    
# 用编码器转换一组标记
labels=['toyota','ford','audi']
encoder_labels = label_encoder.transform(labels)
print("标签：",labels)
print("编码后的标签：",list(encoder_labels))
```

```
Class mapping:
audi --> 0
bmw --> 1
ford --> 2
toyota --> 3
标签： ['toyota', 'ford', 'audi']
编码后的标签： [3, 2, 0]
```

<6.基于朴素贝叶斯算法的分类模型.ipynb>

```python
原标签 = <编码器>.inverse_transform(<编码后的标签>)
label = encoder_list.inverse_transform(label_encoder) # 即可恢复
```



